{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c73efe4",
   "metadata": {},
   "source": [
    "# ML Assignment 1\n",
    "\n",
    "Data set 1 - Bank Account Fraud Dataset \\\n",
    "https://www.kaggle.com/datasets/sgpjesus/bank-account-fraud-dataset-neurips-2022 \\\n",
    "Data set 2 - Customer Segmentation \\\n",
    "https://www.kaggle.com/datasets/abisheksudarshan/customer-segmentation?select=train.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f877a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from datetime import datetime\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from statistics import mean\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import process_time\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44225bd5",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246ddb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, nrows = 5000):\n",
    "    df = pd.read_csv(path, nrows = nrows)\n",
    "    # df = df.dropna()\n",
    "    # df.columns = [i.lower() for i in df.columns]\n",
    "    # df.drop(columns = ['payment_type', 'employment_status', 'housing_status', 'source','device_os'], inplace=True)\n",
    "    return df\n",
    "\n",
    "# df = load_data('Base.csv')\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing value in each column\n",
    "# nacol = pd.DataFrame(df.isnull().sum().sort_values(ascending=False)/len(df))\n",
    "# nacol = nacol[nacol[0]>0]\n",
    "# nacol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd9aae1",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bdc87db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split(df, target = 'fraud_bool'):\n",
    "    # split df to train set and test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, \n",
    "                                                        train_size = 0.8, random_state=0)\n",
    "    \n",
    "    \n",
    "    # split train set to train set and validation set\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, stratify = y_train, \n",
    "                                                    train_size = 0.8, random_state=0)\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "def train_valid_test_split2(df, target = 'Segmentation'):\n",
    "    # split df to train set and test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, \n",
    "                                                        train_size = 0.8, random_state=0)\n",
    "    \n",
    "    \n",
    "    # split train set to train set and validation set\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, stratify = y_train, \n",
    "                                                    train_size = 0.8, random_state=0)\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33881ea0",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea696dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def impute_missing(X_train, X_valid, X_test):\n",
    "#     from sklearn.impute import KNNImputer\n",
    "#     numcols = X_train.select_dtypes(include=['float','int']).columns\n",
    "#     imputer = KNNImputer(n_neighbors=3, weights=\"uniform\")\n",
    "#     imputer.fit(X_train[numcols])\n",
    "#     X_train[numcols] = pd.DataFrame(imputer.transform(X_train[numcols]), columns=numcols)\n",
    "#     X_valid[numcols] = pd.DataFrame(imputer.transform(X_train[numcols]), columns=numcols)\n",
    "#     X_test[numcols] = pd.DataFrame(imputer.transform(X_train[numcols]), columns=numcols)\n",
    "    \n",
    "#     return X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d6369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn impute numeric columns and one hot encode category columns\n",
    "def ohe(X_train, X_valid, X_test):\n",
    "    # impute numeric columns\n",
    "    numcols = X_train.select_dtypes(include=['float','int']).columns\n",
    "    imputer = KNNImputer(n_neighbors=3, weights=\"uniform\")\n",
    "    imputer.fit(X_train[numcols])\n",
    "    X_train[numcols] = pd.DataFrame(imputer.transform(X_train[numcols]), columns=numcols)\n",
    "    X_valid[numcols] = pd.DataFrame(imputer.transform(X_train[numcols]), columns=numcols)\n",
    "    X_test[numcols] = pd.DataFrame(imputer.transform(X_train[numcols]), columns=numcols)\n",
    "    \n",
    "    # impute categorical columns\n",
    "    ohe_training_predictors = pd.get_dummies(X_train)\n",
    "    ohe_valid_predictors = pd.get_dummies(X_valid)\n",
    "    ohe_test_predictors = pd.get_dummies(X_test)\n",
    "    X_train, X_valid = ohe_training_predictors.align(\n",
    "        ohe_valid_predictors,join='left', axis=1)\n",
    "\n",
    "    X_train, X_test = ohe_training_predictors.align(\n",
    "        ohe_test_predictors,join='left', axis=1)\n",
    "    \n",
    "    X_train=X_train.fillna(0)\n",
    "    X_valid=X_valid.fillna(0)\n",
    "    X_test=X_test.fillna(0)\n",
    "    \n",
    "    return X_train, X_valid, X_test\n",
    "\n",
    "# X_train, X_valid, X_test = ohe(X_train, X_valid, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc1d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_catcols(X_train, X_valid, X_test):\n",
    "    catcols = X_train.select_dtypes(include=['object']).columns\n",
    "    X_train.drop(columns = catcols, inplace=True)\n",
    "    X_valid.drop(columns = catcols, inplace=True)\n",
    "    X_test.drop(columns = catcols, inplace=True)\n",
    "    \n",
    "    return X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0034fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversampling(X_train, y_train):\n",
    "    # over-sampling: match the minority class to the majority class\n",
    "    from collections import Counter\n",
    "    from imblearn.over_sampling import (RandomOverSampler, SMOTE, ADASYN)\n",
    "    sampler = RandomOverSampler(sampling_strategy='auto',random_state=0)\n",
    "    X_train_rs, y_train_rs = sampler.fit_resample(X_train, y_train)\n",
    "#     print('Before resample {}'.format(Counter(y_train)),' RandomOverSampler {}'.format(Counter(y_train_rs)))\n",
    "    \n",
    "    return X_train_rs, y_train_rs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745f6040",
   "metadata": {},
   "source": [
    "# Modelling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671db6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(model, X_train, y_train, X_valid, y_valid):\n",
    "    t_start = process_time()\n",
    "    model = model.fit(X_train, y_train)\n",
    "    # predict\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_valid)\n",
    "    t_stop = process_time()\n",
    "    t = t_stop - t_start\n",
    "    \n",
    "    # calculate score\n",
    "    from sklearn.metrics import roc_curve,roc_auc_score, auc, f1_score, accuracy_score, recall_score\n",
    "    train_roc = roc_auc_score(y_train,train_pred)\n",
    "    cv_score = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    test_roc = roc_auc_score(y_valid,test_pred)\n",
    "    f1_score = f1_score(y_valid, test_pred, average='weighted')\n",
    "    acc = accuracy_score(y_valid, test_pred)\n",
    "    recall_score = recall_score(y_valid, test_pred, average='weighted')\n",
    "    \n",
    "    perf = pd.DataFrame({'acc':[round(acc,3)],\n",
    "                         'cv_score':[round(mean(cv_score),3)],\n",
    "                         'train_roc':[round(train_roc,3)],                \n",
    "                         'test_roc':[round(test_roc,3)],\n",
    "                         'recall_score':[round(recall_score,3)],\n",
    "                         'f1_score':[round(f1_score,3)],\n",
    "                         'run_time':[round(t,5)]\n",
    "                        })\n",
    "    return perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e06070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(model, X_train, y_train, X_valid, y_valid):\n",
    "    t_start = process_time()\n",
    "    model = model.fit(X_train, y_train)\n",
    "    # predict\n",
    "    train_pred = pd.DataFrame(model.predict(X_train))\n",
    "    test_pred = pd.DataFrame(model.predict(X_valid))\n",
    "    train_pred_prob = model.predict_proba(X_train)\n",
    "    test_pred_prob = model.predict_proba(X_valid)\n",
    "    t_stop = process_time()\n",
    "    t = t_stop - t_start\n",
    "    # calculate score\n",
    "    from sklearn.metrics import roc_curve,roc_auc_score, auc, f1_score, accuracy_score, recall_score\n",
    "    train_roc = roc_auc_score(y_train,train_pred_prob, multi_class = 'ovr', average = 'weighted')\n",
    "    cv_score = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    test_roc = roc_auc_score(y_valid,test_pred_prob, multi_class = 'ovr', average = 'weighted')\n",
    "    f1_score = f1_score(y_valid, test_pred, average='weighted')\n",
    "    acc = accuracy_score(y_valid, test_pred)\n",
    "    recall_score = recall_score(y_valid, test_pred, average='weighted')\n",
    "    \n",
    "    perf = pd.DataFrame({'acc':[round(acc,3)],\n",
    "                         'cv_score':[round(mean(cv_score),3)],\n",
    "                         'train_roc':[round(train_roc,3)],                \n",
    "                         'test_roc':[round(test_roc,3)],\n",
    "                         'recall_score':[round(recall_score,3)],\n",
    "                         'f1_score':[round(f1_score,3)],\n",
    "                         'run_time':[round(t,3)]\n",
    "                        })\n",
    "    return perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d48a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import tree\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# clf = tree.DecisionTreeClassifier()\n",
    "# clf = clf.fit(X_train, y_train)\n",
    "# print(cross_val_score(estimator=clf, X=X_train, y=y_train, cv=5))\n",
    "# pred = clf.predict(X_valid)\n",
    "# pd.crosstab(pred,y_valid)\n",
    "# # pred\n",
    "# from sklearn.metrics import roc_curve,roc_auc_score, auc\n",
    "# print(roc_auc_score(y_train,clf.predict(X_train)))\n",
    "# print(roc_auc_score(y_valid,clf.predict(X_valid)))\n",
    "# print(roc_auc_score(y_test,clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c56a0d9",
   "metadata": {},
   "source": [
    "# Graph function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47218433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curve_graph(perf_out):\n",
    "    plt.plot(perf_out['train_ratio'], perf_out['recall_score'], label = \"recall_score\")\n",
    "    plt.plot(perf_out['train_ratio'], perf_out['train_roc'], label = \"train_roc\")\n",
    "    plt.plot(perf_out['train_ratio'], perf_out['test_roc'], label = \"test_roc\")\n",
    "    plt.plot(perf_out['train_ratio'], perf_out['f1_score'], label = \"f1_score\")\n",
    "    # plt.plot(le_perf_out['train_ratio'], perf_out['run_time'], label = \"run_time\")\n",
    "    plt.xlabel(\"train data size\")\n",
    "    plt.ylabel(\"score\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cef30bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_graph(clf, X_train):\n",
    "    importances = pd.DataFrame({'features': X_train.columns, 'importance': np.round(clf.feature_importances_,3)})\n",
    "    importances = importances.sort_values('importance',ascending=False)\n",
    "    importances.sort_values('importance', ascending=False)\n",
    "    import seaborn as sns\n",
    "    sns.set(rc={'figure.figsize':(25,15)})\n",
    "    sns.barplot(x='importance', y = 'features', data = importances[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d46548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_graph(clf, X_train, y_train):\n",
    "    fig = plt.figure(figsize=(25,20))\n",
    "    clf.fit\n",
    "    _ = tree.plot_tree(clf, \n",
    "                       feature_names=X_train.columns,  \n",
    "                       class_names=y_train.unique().astype(str),\n",
    "                       filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a7f95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89963fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
